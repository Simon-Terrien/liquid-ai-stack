# ft_trainer/train.py
"""
Fine-tuning trainer for LiquidAI models.

Supports:
- Full fine-tuning (small models)
- LoRA fine-tuning (recommended for 1.2B+)
- QLoRA (4-bit quantized LoRA)

Uses JSONL datasets generated by the ETL pipeline.
"""
import json
import logging
from dataclasses import dataclass, field
from pathlib import Path

import torch
from liquid_shared import (
    DATA_DIR,
    FT_BATCH_SIZE,
    FT_LEARNING_RATE,
    FT_NUM_EPOCHS,
    FT_WARMUP_STEPS,
    LFM_SMALL,
    recommend_device,
)
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from torch.utils.data import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

# MLflow tracking (optional)
try:
    from .mlflow_tracking import create_mlflow_tracker, MLflowCallback
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logger.warning("MLflow tracking not available")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Directories
FT_DIR = DATA_DIR / "ft"
OUTPUT_DIR = DATA_DIR / "ft_output"


@dataclass
class FTConfig:
    """Fine-tuning configuration."""
    # Model
    model_name: str = LFM_SMALL

    # Training
    batch_size: int = FT_BATCH_SIZE
    learning_rate: float = FT_LEARNING_RATE
    num_epochs: int = FT_NUM_EPOCHS
    warmup_steps: int = FT_WARMUP_STEPS
    max_seq_length: int = 2048

    # LoRA
    use_lora: bool = True
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: list[str] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj"]
    )

    # Quantization
    use_4bit: bool = False

    # Output
    output_dir: Path = OUTPUT_DIR

    # Hardware
    fp16: bool = True
    bf16: bool = False

    # MLflow
    use_mlflow: bool = True
    mlflow_experiment: str = "liquid-ft-training"
    mlflow_run_name: str | None = None
    mlflow_tracking_uri: str | None = None


class InstructionDataset(Dataset):
    """
    Dataset for instruction fine-tuning.
    
    Expects JSONL with format:
    {"instruction": "...", "input": "...", "output": "..."}
    """

    def __init__(
        self,
        data_path: Path,
        tokenizer,
        max_length: int = 2048,
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []

        # Load data
        if data_path.is_file():
            self._load_file(data_path)
        elif data_path.is_dir():
            for f in data_path.glob("*.jsonl"):
                self._load_file(f)

        logger.info(f"Loaded {len(self.samples)} training samples")

    def _load_file(self, path: Path):
        with path.open() as f:
            for line in f:
                if line.strip():
                    self.samples.append(json.loads(line))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # Format as instruction
        instruction = sample.get("instruction", "")
        input_text = sample.get("input", "")
        output = sample.get("output", "")

        if input_text:
            prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
        else:
            prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

        # Tokenize
        encoding = self.tokenizer(
            prompt,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": encoding["input_ids"].squeeze(),
        }


def load_model_for_training(config: FTConfig):
    """
    Load model and tokenizer for training.
    
    Returns:
        Tuple of (model, tokenizer)
    """
    logger.info(f"Loading model: {config.model_name}")

    # Get device config
    device_config = recommend_device(config.model_name)
    logger.info(f"Device: {device_config.device}, dtype: {device_config.dtype_name}")

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Model loading kwargs
    model_kwargs = {
        "trust_remote_code": False,
    }

    # Quantization config for QLoRA
    if config.use_4bit:
        from transformers import BitsAndBytesConfig

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        model_kwargs["quantization_config"] = bnb_config
        model_kwargs["device_map"] = "auto"
    elif device_config.device == "cuda":
        model_kwargs["torch_dtype"] = device_config.torch_dtype
        model_kwargs["device_map"] = "auto"

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        **model_kwargs
    )

    # Prepare for training
    if config.use_4bit:
        model = prepare_model_for_kbit_training(model)

    # Add LoRA if configured
    if config.use_lora:
        logger.info("Applying LoRA configuration")

        lora_config = LoraConfig(
            r=config.lora_r,
            lora_alpha=config.lora_alpha,
            lora_dropout=config.lora_dropout,
            target_modules=config.lora_target_modules,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )

        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    return model, tokenizer


def train(
    config: FTConfig,
    data_path: Path | None = None,
    eval_path: Path | None = None,
):
    """
    Run fine-tuning training.

    Args:
        config: Training configuration
        data_path: Path to training data (JSONL file or directory)
        eval_path: Optional path to evaluation data
    """
    # Defaults
    data_path = data_path or FT_DIR
    output_dir = config.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    # Initialize MLflow tracking
    mlflow_tracker = None
    if config.use_mlflow and MLFLOW_AVAILABLE:
        mlflow_tracker = create_mlflow_tracker(
            experiment_name=config.mlflow_experiment,
            run_name=config.mlflow_run_name,
            tracking_uri=config.mlflow_tracking_uri,
        )
        mlflow_tracker.start_run()

        # Log hyperparameters
        mlflow_tracker.log_params({
            "model": config.model_name,
            "batch_size": config.batch_size,
            "learning_rate": config.learning_rate,
            "num_epochs": config.num_epochs,
            "warmup_steps": config.warmup_steps,
            "max_seq_length": config.max_seq_length,
            "use_lora": config.use_lora,
            "lora_r": config.lora_r,
            "lora_alpha": config.lora_alpha,
            "lora_dropout": config.lora_dropout,
            "use_4bit": config.use_4bit,
            "fp16": config.fp16,
            "bf16": config.bf16,
        })

    try:
        # Load model
        model, tokenizer = load_model_for_training(config)

        # Load dataset
        train_dataset = InstructionDataset(
            data_path,
            tokenizer,
            max_length=config.max_seq_length,
        )

        eval_dataset = None
        if eval_path and eval_path.exists():
            eval_dataset = InstructionDataset(
                eval_path,
                tokenizer,
                max_length=config.max_seq_length,
            )

        # Log dataset info
        if mlflow_tracker:
            mlflow_tracker.log_params({
                "train_samples": len(train_dataset),
                "eval_samples": len(eval_dataset) if eval_dataset else 0,
                "data_path": str(data_path),
            })

        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=config.num_epochs,
            per_device_train_batch_size=config.batch_size,
            per_device_eval_batch_size=config.batch_size,
            warmup_steps=config.warmup_steps,
            learning_rate=config.learning_rate,
            fp16=config.fp16 and torch.cuda.is_available(),
            bf16=config.bf16 and torch.cuda.is_bf16_supported(),
            logging_steps=10,
            save_steps=500,
            save_total_limit=3,
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=500 if eval_dataset else None,
            load_best_model_at_end=True if eval_dataset else False,
            report_to="none",  # Disable wandb etc. (using MLflow)
            gradient_accumulation_steps=4,
            gradient_checkpointing=True,
            optim="adamw_torch",
        )

        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )

        # Callbacks
        callbacks = []
        if mlflow_tracker:
            callbacks.append(MLflowCallback(mlflow_tracker))

        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            callbacks=callbacks,
        )

        # Train
        logger.info("Starting training...")
        train_result = trainer.train()

        # Log final training metrics
        if mlflow_tracker:
            mlflow_tracker.log_metrics({
                "final_train_loss": train_result.training_loss,
                "total_epochs": config.num_epochs,
            })

        # Save final model
        final_path = output_dir / "final"
        trainer.save_model(str(final_path))
        tokenizer.save_pretrained(str(final_path))

        # Log model to MLflow
        if mlflow_tracker:
            mlflow_tracker.log_model(model, "final_model")
            mlflow_tracker.log_artifact(final_path, "model_checkpoint")

        logger.info(f"Training complete! Model saved to: {final_path}")

        if mlflow_tracker:
            mlflow_tracker.end_run(status="FINISHED")

        return final_path

    except Exception as e:
        logger.error(f"Training failed: {e}")
        if mlflow_tracker:
            mlflow_tracker.end_run(status="FAILED")
        raise


def main():
    """CLI entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Fine-tune LiquidAI models")
    parser.add_argument(
        "--model",
        default=LFM_SMALL,
        help="Model to fine-tune"
    )
    parser.add_argument(
        "--data",
        type=Path,
        default=FT_DIR,
        help="Training data path"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=OUTPUT_DIR,
        help="Output directory"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=FT_NUM_EPOCHS,
        help="Number of epochs"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=FT_BATCH_SIZE,
        help="Batch size"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=FT_LEARNING_RATE,
        help="Learning rate"
    )
    parser.add_argument(
        "--lora",
        action="store_true",
        default=True,
        help="Use LoRA (default: True)"
    )
    parser.add_argument(
        "--no-lora",
        action="store_false",
        dest="lora",
        help="Disable LoRA for full fine-tuning"
    )
    parser.add_argument(
        "--4bit",
        action="store_true",
        dest="use_4bit",
        help="Use 4-bit quantization (QLoRA)"
    )

    args = parser.parse_args()

    config = FTConfig(
        model_name=args.model,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        num_epochs=args.epochs,
        use_lora=args.lora,
        use_4bit=args.use_4bit,
        output_dir=args.output,
    )

    train(config, data_path=args.data)


if __name__ == "__main__":
    main()
