# ft_trainer/train.py
"""
Fine-tuning trainer for LiquidAI models.

Supports:
- Full fine-tuning (small models)
- LoRA fine-tuning (recommended for 1.2B+)
- QLoRA (4-bit quantized LoRA)

Uses JSONL datasets generated by the ETL pipeline.
"""
import json
import logging
from dataclasses import dataclass, field
from pathlib import Path

import torch
from liquid_shared import (
    DATA_DIR,
    FT_BATCH_SIZE,
    FT_LEARNING_RATE,
    FT_NUM_EPOCHS,
    FT_WARMUP_STEPS,
    LFM_SMALL,
    recommend_device,
)
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from torch.utils.data import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Directories
FT_DIR = DATA_DIR / "ft"
OUTPUT_DIR = DATA_DIR / "ft_output"


@dataclass
class FTConfig:
    """Fine-tuning configuration."""
    # Model
    model_name: str = LFM_SMALL

    # Training
    batch_size: int = FT_BATCH_SIZE
    learning_rate: float = FT_LEARNING_RATE
    num_epochs: int = FT_NUM_EPOCHS
    warmup_steps: int = FT_WARMUP_STEPS
    max_seq_length: int = 2048

    # LoRA
    use_lora: bool = True
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: list[str] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj"]
    )

    # Quantization
    use_4bit: bool = False

    # Output
    output_dir: Path = OUTPUT_DIR

    # Hardware
    fp16: bool = True
    bf16: bool = False


class InstructionDataset(Dataset):
    """
    Dataset for instruction fine-tuning.
    
    Expects JSONL with format:
    {"instruction": "...", "input": "...", "output": "..."}
    """

    def __init__(
        self,
        data_path: Path,
        tokenizer,
        max_length: int = 2048,
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []

        # Load data
        if data_path.is_file():
            self._load_file(data_path)
        elif data_path.is_dir():
            for f in data_path.glob("*.jsonl"):
                self._load_file(f)

        logger.info(f"Loaded {len(self.samples)} training samples")

    def _load_file(self, path: Path):
        with path.open() as f:
            for line in f:
                if line.strip():
                    self.samples.append(json.loads(line))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # Format as instruction
        instruction = sample.get("instruction", "")
        input_text = sample.get("input", "")
        output = sample.get("output", "")

        if input_text:
            prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
        else:
            prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

        # Tokenize
        encoding = self.tokenizer(
            prompt,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": encoding["input_ids"].squeeze(),
        }


def load_model_for_training(config: FTConfig):
    """
    Load model and tokenizer for training.
    
    Returns:
        Tuple of (model, tokenizer)
    """
    logger.info(f"Loading model: {config.model_name}")

    # Get device config
    device_config = recommend_device(config.model_name)
    logger.info(f"Device: {device_config.device}, dtype: {device_config.dtype_name}")

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Model loading kwargs
    model_kwargs = {
        "trust_remote_code": False,
    }

    # Quantization config for QLoRA
    if config.use_4bit:
        from transformers import BitsAndBytesConfig

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        model_kwargs["quantization_config"] = bnb_config
        model_kwargs["device_map"] = "auto"
    elif device_config.device == "cuda":
        model_kwargs["torch_dtype"] = device_config.torch_dtype
        model_kwargs["device_map"] = "auto"

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        **model_kwargs
    )

    # Prepare for training
    if config.use_4bit:
        model = prepare_model_for_kbit_training(model)

    # Add LoRA if configured
    if config.use_lora:
        logger.info("Applying LoRA configuration")

        lora_config = LoraConfig(
            r=config.lora_r,
            lora_alpha=config.lora_alpha,
            lora_dropout=config.lora_dropout,
            target_modules=config.lora_target_modules,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )

        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    return model, tokenizer


def train(
    config: FTConfig,
    data_path: Path | None = None,
    eval_path: Path | None = None,
):
    """
    Run fine-tuning training.
    
    Args:
        config: Training configuration
        data_path: Path to training data (JSONL file or directory)
        eval_path: Optional path to evaluation data
    """
    # Defaults
    data_path = data_path or FT_DIR
    output_dir = config.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load model
    model, tokenizer = load_model_for_training(config)

    # Load dataset
    train_dataset = InstructionDataset(
        data_path,
        tokenizer,
        max_length=config.max_seq_length,
    )

    eval_dataset = None
    if eval_path and eval_path.exists():
        eval_dataset = InstructionDataset(
            eval_path,
            tokenizer,
            max_length=config.max_seq_length,
        )

    # Training arguments
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        warmup_steps=config.warmup_steps,
        learning_rate=config.learning_rate,
        fp16=config.fp16 and torch.cuda.is_available(),
        bf16=config.bf16 and torch.cuda.is_bf16_supported(),
        logging_steps=10,
        save_steps=500,
        save_total_limit=3,
        evaluation_strategy="steps" if eval_dataset else "no",
        eval_steps=500 if eval_dataset else None,
        load_best_model_at_end=True if eval_dataset else False,
        report_to="none",  # Disable wandb etc.
        gradient_accumulation_steps=4,
        gradient_checkpointing=True,
        optim="adamw_torch",
    )

    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )

    # Train
    logger.info("Starting training...")
    trainer.train()

    # Save final model
    final_path = output_dir / "final"
    trainer.save_model(str(final_path))
    tokenizer.save_pretrained(str(final_path))

    logger.info(f"Training complete! Model saved to: {final_path}")

    return final_path


def main():
    """CLI entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Fine-tune LiquidAI models")
    parser.add_argument(
        "--model",
        default=LFM_SMALL,
        help="Model to fine-tune"
    )
    parser.add_argument(
        "--data",
        type=Path,
        default=FT_DIR,
        help="Training data path"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=OUTPUT_DIR,
        help="Output directory"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=FT_NUM_EPOCHS,
        help="Number of epochs"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=FT_BATCH_SIZE,
        help="Batch size"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=FT_LEARNING_RATE,
        help="Learning rate"
    )
    parser.add_argument(
        "--lora",
        action="store_true",
        default=True,
        help="Use LoRA (default: True)"
    )
    parser.add_argument(
        "--no-lora",
        action="store_false",
        dest="lora",
        help="Disable LoRA for full fine-tuning"
    )
    parser.add_argument(
        "--4bit",
        action="store_true",
        dest="use_4bit",
        help="Use 4-bit quantization (QLoRA)"
    )

    args = parser.parse_args()

    config = FTConfig(
        model_name=args.model,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        num_epochs=args.epochs,
        use_lora=args.lora,
        use_4bit=args.use_4bit,
        output_dir=args.output,
    )

    train(config, data_path=args.data)


if __name__ == "__main__":
    main()
