# LiquidAI Stack - Domain Knowledge Base

This file serves as a knowledge repository for the LiquidAI Stack project.

## Model Information

### LFM2 Model Series

**LFM2-700M**
- Parameters: ~742M
- VRAM Required: 4GB minimum
- Recommended dtype: bf16 or fp16
- Use cases: Fast inference, validation, RAG runtime
- Context window: 32,768 tokens

**LFM2-1.2B**
- Parameters: ~1.17B
- VRAM Required: 6GB minimum
- Recommended dtype: bf16 or fp16
- Use cases: Balanced tasks, summarization, rewriting
- Context window: 32,768 tokens

**LFM2-2.6B**
- Parameters: ~2.57B
- VRAM Required: 10GB minimum
- Recommended dtype: fp16
- Use cases: Quality-focused tasks, chunking, metadata extraction, QA generation
- Context window: 32,768 tokens

## Architecture Patterns

### Multi-Model Orchestration

The stack uses different models for different stages of the ETL pipeline:
1. **Chunking** (2.6B) - Highest quality for semantic splitting
2. **Metadata Extraction** (2.6B) - Complex analysis of chunk content
3. **Summarization** (1.2B) - Balanced quality/speed
4. **QA Generation** (2.6B) - High-quality question-answer pairs
5. **Validation** (700M) - Fast quality filtering
6. **RAG Inference** (700M) - Fast question answering

### Device Selection Strategy

Automatic device selection based on:
- Model size (inferred from model name)
- Available GPU VRAM
- CUDA availability
- bf16 support

Priority: GPU with appropriate dtype > CPU with fp32

### Graph-Based ETL

Uses Pydantic Graph (beta) for orchestration:
- Async step definitions
- Sync agent implementations
- Shared state object
- Error collection (non-failing)
- Parallel execution where possible

## Key Design Decisions

### Two Birds, One Stone ETL

While processing documents for RAG, simultaneously:
1. Create vector embeddings for retrieval
2. Generate fine-tuning datasets (QA pairs)

Benefits:
- Single pass through documents
- Consistent quality across outputs
- Efficient resource usage

### Type Safety First

All data structures use Pydantic models:
- Compile-time type checking
- Runtime validation
- Clear API contracts
- Easy serialization/deserialization

### Sandboxed Operations

Security measures:
- File operations restricted to /data/
- Models loaded with trust_remote_code=False
- SafeTensors format for weights
- No secrets in environment variables

## Performance Characteristics

### Memory Requirements (Estimated)

Per model during inference:
- 700M: ~3GB GPU / ~6GB CPU
- 1.2B: ~5GB GPU / ~10GB CPU
- 2.6B: ~10GB GPU / ~20GB CPU

### Processing Speed (Approximate)

On RTX 4090 (16GB VRAM):
- Chunking: ~50-100 chunks/minute (2.6B)
- Metadata extraction: ~30-60 chunks/minute (2.6B)
- Summarization: ~60-120 chunks/minute (1.2B)
- QA generation: ~20-40 pairs/minute (2.6B)
- RAG inference: ~100-200 queries/minute (700M)

### Bottlenecks

1. **Model loading time**: 5-30 seconds per model
2. **Vector embedding**: Depends on batch size
3. **Disk I/O**: For large document sets

## Common Issues & Solutions

### CUDA OOM
- Solution: Use smaller model variant
- Solution: Reduce batch size
- Solution: Enable gradient checkpointing (for training)

### Slow ETL
- Solution: Process documents in parallel
- Solution: Use CPU for validation step
- Solution: Increase batch size for embeddings

### Vector Store Performance
- Solution: Increase Chroma batch size
- Solution: Use hybrid search (semantic + keyword)
- Solution: Adjust top_k based on use case

## Best Practices

### Document Preparation
- Clean text before ETL
- Remove low-value content (headers, footers)
- Split very large documents

### RAG Configuration
- Use fast_mode=True for interactive queries
- Adjust max_context_chunks based on complexity
- Enable hybrid_search for better recall

### Fine-Tuning
- Filter QA pairs with quality_score >= 0.6
- Balance dataset across topics
- Include metadata in samples

## Technology Stack

### Core Dependencies
- pydantic 2.12+ (type safety)
- pydantic-ai 1.27+ (agent framework)
- torch 2.0+ (model runtime)
- transformers 4.40+ (HF models)
- chromadb 0.4+ (vector store)
- fastapi 0.100+ (API server)

### Development Tools
- uv (package management)
- ruff (linting)
- black (formatting)
- mypy (type checking)
- pytest (testing)

## Extension Points

### Adding New Agents
1. Define input/output schemas in schemas.py
2. Implement agent logic with Pydantic AI
3. Add step to graph_etl.py
4. Wire edges in build_etl_graph()

### Custom Retrieval
1. Implement retrieval function in tools/retrieval.py
2. Register as Pydantic AI tool
3. Add to RAG agent definition

### Alternative Models
1. Update config.py with model IDs
2. Add size detection in devices.py
3. Test device recommendations

## Future Enhancements

- [ ] Parallel document processing
- [ ] Streaming RAG responses
- [ ] Multi-modal support (images, tables)
- [ ] Advanced reranking
- [ ] Query expansion
- [ ] Answer validation
- [ ] Monitoring and metrics
- [ ] A/B testing framework
